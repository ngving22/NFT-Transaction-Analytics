# -*- coding: utf-8 -*-
"""NFT-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FsCWOrtnGqL-3QR1GG3rwKS5jrQoUCJB

**Importing original CSV to make the Dataframe**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#preparing for regression
# import seaborn as sns
# from sklearn import preprocessing, svm
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/rarible_exchanges_clean.csv")

df = df.astype( {"assetAmount": float,'assetAddress': str} )

# royalties = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'ROYALTY')]['assetAmount'].sum()
# payouts = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'PAYOUT')]['assetAmount'].sum()

# print( f"Number of payouts = {len(df.loc[df['transferType'] == 'PAYOUT'])}" )
# print( f"Number of royalty payments = {len(df.loc[df['transferType'] == 'ROYALTY'])}" )

# print( f"Number of ETH payouts = {len(df.loc[(df['transferType'] == 'PAYOUT')&(df['assetType']=='ETH')])}" )
# print( f"Number of non-ETH payouts = {len(df.loc[(df['transferType'] == 'PAYOUT')&(df['assetType']!='ETH')])}" )
# print( f"Number of ETH royalty payments = {len(df.loc[(df['transferType'] == 'ROYALTY')&(df['assetType']=='ETH')])}" )
# print( f"Number of non-ETH royalty payments = {len(df.loc[(df['transferType'] == 'ROYALTY')&(df['assetType'] != 'ETH')])}" )

# print( f"ETH payouts = {payouts}" )
# print( f"ETH royalties = {royalties}" )
# print( f"Percent royalties = {royalties/payouts}" )

# ##############################

# df2 = pd.DataFrame( { 'assetAddress': df.assetAddress.unique() } )
# totals = df.groupby(['assetAddress','transferType','assetType']).agg({"assetAmount":"sum"})

# payouts = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'PAYOUT')][['to','assetAmount']].groupby('to').agg({'assetAmount':'sum'}).rename(columns={'assetAmount':'PAYOUT'},inplace=False).sort_values(by='PAYOUT',ascending=False)
# royalties = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'ROYALTY')][['to','assetAmount']].groupby('to').agg({'assetAmount':'sum'}).rename(columns={'assetAmount':'ROYALTY'},inplace=False).sort_values(by='ROYALTY',ascending=False)
# numpayments = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'PAYMENT')][['to','assetAmount']].groupby('to').agg({'assetAmount':'count'}).rename(columns={'assetAmount':'PAYOUT'},inplace=False).sort_values(by='PAYOUT',ascending=False)
# numroyalties = df.loc[(df['assetType']=='ETH') & (df['transferType'] == 'ROYALTY')][['to','assetAmount']].groupby('to').agg({'assetAmount':'count'}).rename(columns={'assetAmount':'ROYALTY'},inplace=False).sort_values(by='ROYALTY',ascending=False)

# # payouts = df.loc[([df['transferType']=='PAYOUT'])&(df['assetType']=='ETH')]#.groupby('address').agg({'assetAmount':'sum'})[['address','assetAmount']]


# hist = royalties.hist(bins=np.exp( np.multiply( np.log(10), np.linspace(0,20,40) ) ) )
# plt.savefig("total_royalties.png")
# print( royalties.head() )
# print( royalties.tail() )

# hist = numroyalties.hist(bins=20)
# plt.savefig("count_royalties.png")

# print( numroyalties.head() )
# #df2 = df2.merge( totals.loc[totals.asset

df.shape[0]

df.tail()

"""**Studying the reOccurrence in "to-address" column**"""

df.to.value_counts().head(50)

df["from"].value_counts()

df['to'].value_counts().loc[lambda x: x>10000]

value_counts_to = df['to'].value_counts()

df_to = pd.DataFrame(value_counts_to)
df_to = df_to.reset_index()
df_to.columns = ['address', 'Occurrence']
df_to

"""**At this point, we have 59897 unique to address out of the 488556 transaction**"""

df_balance = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/balance_info.csv")

df_balance

df_to_balance = pd.merge(df_to, df_balance, how="outer")

df_to_balance = df_to_balance.apply(pd.to_numeric, errors = 'coerce')

df3 = pd.DataFrame(df_to_balance.corr())

df3

df3.to_csv('currency_occurrence_correlation.csv')

df_to_balance['ETH'].astype(np.float64)
df_to_balance['Occurrence'].astype(np.float64)

column_1 = df_to_balance['ETH'].astype(np.float64)
column_2 = df_to_balance['Occurrence'].astype(np.float64)
ETH_correlation = column_1.corr(column_2)
print(ETH_correlation)

column_1 = df_to_balance['WETH'].astype(np.float64)
column_2 = df_to_balance['Occurrence'].astype(np.float64)
WETH_correlation = column_1.corr(column_2)
print(WETH_correlation)

column_1 = df_to_balance['RARI'].astype(np.float64)
column_2 = df_to_balance['Occurrence'].astype(np.float64)
RARI_correlation = column_1.corr(column_2)
print(RARI_correlation)

column_1 = df_to_balance['USDC'].astype(np.float64)
column_2 = df_to_balance['Occurrence'].astype(np.float64)
USDC_correlation = column_1.corr(column_2)
print(USDC_correlation)











df_to_balance['ETH_base10'] = np.log10(df_to_balance['ETH'].astype(np.float64))

df_to_balance['Occurrence_base2'] = np.log2(df_to_balance['Occurrence'].astype(np.float64))

df_to_balance

df_binary = df_to_balance[["ETH_base10", 'Occurrence_base2']]
df_binary.head()

df_binary.replace([np.inf, -np.inf], np.nan, inplace=True)
df_binary.dropna(subset=["ETH_base10", 'Occurrence_base2'], how="all", inplace=True)

sns.lmplot(x ="ETH_base10", y ="Occurrence_base2", data = df_binary, order = 2, ci = None)

X = np.array(df_binary['ETH_base10']).reshape(-1, 1)
y = np.array(df_binary['Occurrence_base2']).reshape(-1, 1)
 
# Separating the data into independent and dependent variables
# Converting each dataframe into a numpy array
# since each dataframe contains only one column
 
# Dropping any rows with Nan values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
 
# Splitting the data into training and testing data
regr = LinearRegression()
 
regr.fit(X_train, y_train)
print(regr.score(X_test, y_test))

"""**Since the linear correlation is low, we attempt the same analysis with smaller data set.**

**Top 500**
"""

df_binary500 = df_binary[:][:500]
   
# Selecting the 1st 500 rows of the data
sns.lmplot(x ="ETH_base10", y ="Occurrence_base2", data = df_binary500,
                               order = 2, ci = None)

df_binary500.fillna(method ='ffill', inplace = True)
 
y = np.array(df_binary500['Occurrence_base2']).reshape(-1, 1)
X = np.array(df_binary500['ETH_base10']).reshape(-1, 1)
 
df_binary500.dropna(inplace = True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
 
regr = LinearRegression()
regr.fit(X_train, y_train)
print(regr.score(X_test, y_test))

"""**Top 50**"""

df_binary50 = df_binary[:][:50]
   
# Selecting the 1st 50 rows of the data
sns.lmplot(x ="ETH_base10", y ="Occurrence_base2", data = df_binary50,
                               order = 2, ci = None)

df_binary50.fillna(method ='ffill', inplace = True)
 
y = np.array(df_binary50['Occurrence_base2']).reshape(-1, 1)
X = np.array(df_binary50['ETH_base10']).reshape(-1, 1)
 
df_binary50.dropna(inplace = True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
 
regr = LinearRegression()
regr.fit(X_train, y_train)
print(regr.score(X_test, y_test))



from sklearn.metrics import mean_absolute_error,mean_squared_error
 
mae = mean_absolute_error(y_true=y_test,y_pred=y_pred)
#squared True returns MSE value, False returns RMSE value.
mse = mean_squared_error(y_true=y_test,y_pred=y_pred) #default=True
rmse = mean_squared_error(y_true=y_test,y_pred=y_pred,squared=False)
 
print("MAE:",mae)
print("MSE:",mse)
print("RMSE:",rmse)